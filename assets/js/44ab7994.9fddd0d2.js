"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[763],{3905:function(e,t,n){n.d(t,{Zo:function(){return p},kt:function(){return d}});var a=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var o=a.createContext({}),c=function(e){var t=a.useContext(o),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},p=function(e){var t=c(e.components);return a.createElement(o.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),m=c(n),d=r,g=m["".concat(o,".").concat(d)]||m[d]||u[d]||i;return n?a.createElement(g,s(s({ref:t},p),{},{components:n})):a.createElement(g,s({ref:t},p))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,s=new Array(i);s[0]=m;var l={};for(var o in t)hasOwnProperty.call(t,o)&&(l[o]=t[o]);l.originalType=e,l.mdxType="string"==typeof e?e:r,s[1]=l;for(var c=2;c<i;c++)s[c]=n[c];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9018:function(e,t,n){n.r(t),n.d(t,{assets:function(){return p},contentTitle:function(){return o},default:function(){return d},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return u}});var a=n(7462),r=n(3366),i=(n(7294),n(3905)),s=["components"],l={sidebar_position:2},o="Clientgen",c={unversionedId:"software/clientgen",id:"software/clientgen",title:"Clientgen",description:"An open source stateful PTP client traffic generator based on PF_RING and simpleclient example.",source:"@site/docs/software/clientgen.md",sourceDirName:"software",slug:"/software/clientgen",permalink:"/Time-Appliance-Project/docs/software/clientgen",draft:!1,tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"The Firmware Tool",permalink:"/Time-Appliance-Project/docs/software/tft"},next:{title:"Experimental Software",permalink:"/Time-Appliance-Project/docs/software/experimental"}},p={},u=[{value:"Operation details",id:"operation-details",level:2},{value:"Configuration",id:"configuration",level:3},{value:"Operation",id:"operation",level:3},{value:"Incoming packets",id:"incoming-packets",level:3},{value:"Outgoing packets",id:"outgoing-packets",level:3},{value:"Client processing",id:"client-processing",level:3},{value:"Installation guide",id:"installation-guide",level:2},{value:"PF_RING",id:"pf_ring",level:3},{value:"Pull clientgen",id:"pull-clientgen",level:3},{value:"Build clientgen",id:"build-clientgen",level:3},{value:"Usage guide",id:"usage-guide",level:2},{value:"Traffic and client configuration",id:"traffic-and-client-configuration",level:3},{value:"Performance controls",id:"performance-controls",level:3},{value:"Debug logging",id:"debug-logging",level:3},{value:"Periodic statistics printing",id:"periodic-statistics-printing",level:3},{value:"Example CLI output",id:"example-cli-output",level:2}],m={toc:u};function d(e){var t=e.components,n=(0,r.Z)(e,s);return(0,i.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"clientgen"},"Clientgen"),(0,i.kt)("p",null,"An open source stateful PTP client traffic generator based on PF_RING and simpleclient example."),(0,i.kt)("h2",{id:"operation-details"},"Operation details"),(0,i.kt)("p",null,"This utility is designed to simulate large number of PTP clients that go through the following sequence"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Request Sync Unicast Grant for a specified duration"),(0,i.kt)("li",{parentName:"ol"},"Request Announce Unicast Grant for a specified duration"),(0,i.kt)("li",{parentName:"ol"},"Request Delay Response Unicast Grant for a specified duration"),(0,i.kt)("li",{parentName:"ol"},"Periodically send DelayResp requests to the server while the Grants are active"),(0,i.kt)("li",{parentName:"ol"},"Potentially restart after all the Grants have expired")),(0,i.kt)("h3",{id:"configuration"},"Configuration"),(0,i.kt)("p",null,"The utility is configured through a single json configuration file, detailed below, clientgen_config.json"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The clients are specified by IP address start, IP address end, and IP address step size, similar to commercial traffic generators")),(0,i.kt)("h3",{id:"operation"},"Operation"),(0,i.kt)("p",null,"The utility runs via CLI , and provides printed output periodically (period specified in config) with detailed output, configurable based on the config."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"An example of the CLI output is below. Each section printed can be disabled if desired in the config.")),(0,i.kt)("h3",{id:"incoming-packets"},"Incoming packets"),(0,i.kt)("p",null,"The utility is based on pipelining incoming packets in the following manner:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"RX ioWkr: Read an incoming packet from PF_RING buffer (including RX timestamp), pass it to packetParser"),(0,i.kt)("li",{parentName:"ol"},"packetParser: Parse the incoming packet using gopacket to decode the packet layers, pass it to packetProcessor"),(0,i.kt)("li",{parentName:"ol"},"packetProcessor:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"If packet is ARP (for IPv4) or ICMPv6 (for IPv6), craft a response based on client configuration. Pass response to txWkr"),(0,i.kt)("li",{parentName:"ul"},"if packet is UDP , look into the UDP packet to figure out if it's for a simulated client and craft a response if needed. Pass response to txWkr."),(0,i.kt)("li",{parentName:"ul"},"If packet was from TX path (see below) , then this packet is just used to figure out where to store the timestamp that came with it (TX timestamp)")))),(0,i.kt)("p",null,"The utility works using low level packet crafting and packet capture to control each individual packet sent and handle each packet received."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Because each packet is crafted, the utility requires pre-knowledge about the DUT MAC address in the config file"),(0,i.kt)("li",{parentName:"ul"},"The utility also requires the name of the interface, like ens1f0, in the config file to bind the sockets and pcap library to"),(0,i.kt)("li",{parentName:"ul"},"PF_RING is used for packet ingress.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"It allows all incoming packets on an interface to be round-robin distributed to an arbitrary number of worker goroutines"),(0,i.kt)("li",{parentName:"ul"},"It allows for much larger packet buffering, buffering packets in the CPU memory rather than NIC buffer spaces"),(0,i.kt)("li",{parentName:"ul"},"It removes polling the NIC from the user space, instead directly polling the NIC in the kernel and buffering them packets in user space buffers")))),(0,i.kt)("h3",{id:"outgoing-packets"},"Outgoing packets"),(0,i.kt)("p",null,"The utility has txWkrs to handle sending packets:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Each packet sent to the txWkr has a flag if it needs timestamping or not"),(0,i.kt)("li",{parentName:"ul"},"If timestamping is not required, the packet is sent with a simple socket"),(0,i.kt)("li",{parentName:"ul"},"If timestamping is required, the packet is sent on a socket with timestamping enabled."),(0,i.kt)("li",{parentName:"ul"},"Each txWkr has multiple goroutines polling its timestamped enabled socket to pull TX timestamps as soon as possible"),(0,i.kt)("li",{parentName:"ul"},"For each TX timestamp, the full packet sent is also read back. This packet is passed to packetParser with a flag indicating it was a sent packet")),(0,i.kt)("h3",{id:"client-processing"},"Client processing"),(0,i.kt)("p",null,"The utility has retransmitWorkers to handle retransmitting packets:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Each retransmitWorker maintains a heap based on time to specify when a client should be retransmitted if no response came in."),(0,i.kt)("li",{parentName:"ul"},"Each heap item is associated with a particular client."),(0,i.kt)("li",{parentName:"ul"},"This heap is used to retransmit grants. For example, if a client did not get Sync Grant from the server within a time, this processor will retransmit Sync Grant Request"),(0,i.kt)("li",{parentName:"ul"},"It's also used to retransmit if needed. For example, when a client has all the grants, it is used to send DelayReq periodically.")),(0,i.kt)("p",null,"The utility has restartWorkers to handle restarting clients:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Each restartWorker maintains a heap based on time if restart is enabled in the config"),(0,i.kt)("li",{parentName:"ul"},"When a client receives all three grants from the server, it pushes itself onto the heap to restart after all grants have expired.")),(0,i.kt)("h2",{id:"installation-guide"},"Installation guide"),(0,i.kt)("h3",{id:"pf_ring"},"PF_RING"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Download and install pf_ring from ntop.org")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"git clone https://github.com/ntop/PF_RING.git\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Build the pf_ring kernel module")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"cd PF_RING/kernel\nmake\nsudo make install\ninsmod ./pf_ring.ko\n")),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},"Build and install the API libraries for pf_ring")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"cd PF_RING/userland/lib\n./configure && make\nsudo make install\ncd ../libpcap\n./configure && make\nsudo make install\n")),(0,i.kt)("p",null,"More details can be found here ","*"," ",(0,i.kt)("a",{parentName:"p",href:"https://www.ntop.org/guides/pf_ring/get_started/git_installation.html"},"https://www.ntop.org/guides/pf_ring/get_started/git_installation.html")),(0,i.kt)("h3",{id:"pull-clientgen"},"Pull clientgen"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"git clone https://github.com/opencomputeproject/Time-Appliance-Project\n")),(0,i.kt)("h3",{id:"build-clientgen"},"Build clientgen"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Be sure to link LDD to where the pf_ring libraries are")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/\n")),(0,i.kt)("ol",{start:2},(0,i.kt)("li",{parentName:"ol"},"Be sure to provide path to pf_ring header file. Change this path to where PF_RING was built.")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"C_INCLUDE_PATH=~/ptp/PF_RING/kernel; export C_INCLUDE_PATH\n")),(0,i.kt)("ol",{start:3},(0,i.kt)("li",{parentName:"ol"},"Go to clientgen directory and build")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"cd Software/Experimental/clientgen/\ngo build\n")),(0,i.kt)("h2",{id:"usage-guide"},"Usage guide"),(0,i.kt)("p",null,"Clientgen is configured by a json config file, clientgen_config.json, in the same directory as the executable. Run it by running Clientgen in the clientgen directory. Change the json configuration prior to running to console the execution."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},"cd clientgen/\n./clientgen -config clientgen_config.json [-profilelog <cpuprofiler file>]\n")),(0,i.kt)("p",null,"Description of items in this json file:"),(0,i.kt)("h3",{id:"traffic-and-client-configuration"},"Traffic and client configuration"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},'"Iface" - the interface on the server to generate client traffic from, ex. "ens1f0"'),(0,i.kt)("li",{parentName:"ul"},'"ServerMAC" - the MAC address of the PTP Grandmaster Server, ex. "0c:42:a1:80:31:66"'),(0,i.kt)("li",{parentName:"ul"},'"ServerAddress" - the IPv4 or IPv6 address of the PTP Grandmaster Server, ex. "10.254.254.254"'),(0,i.kt)("li",{parentName:"ul"},'"ClientIPStart" - IPv4 or IPv6. For the range of clients to generate, this is the IP address of the first client. ex. "10.1.1.2"'),(0,i.kt)("li",{parentName:"ul"},'"ClientIPEnd" - IPv4 or IPv6. For the range of clients to generate, this is the last client IP. ex. "10.1.1.10"'),(0,i.kt)("li",{parentName:"ul"},'"ClientIPStep" - For the clients to generate, how much to increment from ClientIPStart for each client. If ClientIPStart is 10.1.1.2 and this is 2, then it will generate clients 10.1.1.2 -> 10.1.1.4 -> 10.1.1.6 -> 10.1.1.8 etc. until ClientIPEnd'),(0,i.kt)("li",{parentName:"ul"},'"SoftStartRate" - The maximum number of clients to start per second'),(0,i.kt)("li",{parentName:"ul"},'"TimeoutSec" - how many seconds to run clientgen for, after which the program will stop generating traffic.'),(0,i.kt)("li",{parentName:"ul"},'"DurationSec" - The Grant Duration each client tries to subscribe to the PTP grandmaster when asking for UDP Grants, like Sync/Announce/DelayResp.'),(0,i.kt)("li",{parentName:"ul"},'"TimeAfterDurationBeforeRestartSec" - Time after a client\'s last grant expires to wait before restarting the client in seconds'),(0,i.kt)("li",{parentName:"ul"},'"TimeBetweenDelayReqSec" - After a client has all its grants, how many DelayReqs to send per second'),(0,i.kt)("li",{parentName:"ul"},'"ClientRetranTimeWhenNoResponseSec" - How many seconds a client should wait when requesting a grant before retransmitting the grant request if no response is received in seconds')),(0,i.kt)("h3",{id:"performance-controls"},"Performance controls"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},'"NumTXWorkers" - How many goroutines to spawn to handle transmitting packets. This may be the main bottleneck, due to TX Timestamping performance.'),(0,i.kt)("li",{parentName:"ul"},'"NumTXTSWorkerPerTx" - How many goroutines to spawn per TX worker to read TX timestamps.'),(0,i.kt)("li",{parentName:"ul"},'"NumRXWorkers" - How many goroutines to spawn to handle RX work.'),(0,i.kt)("li",{parentName:"ul"},'"NumPacketParsers" - How many goroutines to spawn to Parse each received packet into a gopacket format for internal use.'),(0,i.kt)("li",{parentName:"ul"},'"NumPacketProcessors" - How many goroutines to spawn to handle each Parsed packet and possibly generate a response.'),(0,i.kt)("li",{parentName:"ul"},'"NumClientRetransmitProcs" - How many goroutines to spawn to manage internal timers for possible retransmit for each client. Work per goroutine will scale with client count.'),(0,i.kt)("li",{parentName:"ul"},'"NumClientRestartProcs" - How many goroutines to spawn to manage internal timers for restarting clients after their grants are over. Work per goroutine will scale with client count.')),(0,i.kt)("h3",{id:"debug-logging"},"Debug logging"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Enable these only for development and debug purposes, should be left false in almost all cases.")),(0,i.kt)("h3",{id:"periodic-statistics-printing"},"Periodic statistics printing"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},'"PrintPerformance" - Prints the percentage each worker goroutine is busy. Use this to help fine tune the Performance Controls above to get the desired performance.'),(0,i.kt)("li",{parentName:"ul"},'"PrintClientData" - Prints information about all the clients, such as Total Announce Requests sent and Total Announce Grants received'),(0,i.kt)("li",{parentName:"ul"},'"PrintTxRxCounts" - Prints simple TX and RX packet counters'),(0,i.kt)("li",{parentName:"ul"},'"PrintClientReqData" - Prints histogram information for Announce Requests / Sync Requests / Delay Response Grant Requests / Delay Requests for all the clients'),(0,i.kt)("li",{parentName:"ul"},'"PrintLatencyData" - Prints statistical information for the server latency when responding to Announce Requests / Sync Requests / Delay Response Grant Requests / Delay Requests , and statistical information on the time between Sync packets from the grandmaster.'),(0,i.kt)("li",{parentName:"ul"},'"CounterPrintIntervalSecs" - How often to print the enabled statistics')),(0,i.kt)("h2",{id:"example-cli-output"},"Example CLI output"),(0,i.kt)("p",null,"When running with everything printing, this is an example of what the output statistics encompass."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-console"},'========================Statistics after 999.684ms============\n==ClientData=============\n0: TotalClients = 228, rate 0\n1: TotalPacketsSent = 912, rate 912\n2: TotalPacketsRcvd = 1596, rate 1596\n3: TotalTXTSPacketsSent = 912, rate 912\n4: TotalTXTSRead = 912, rate 912\n5: MaxTXTSBytesOutstanding = 3332, rate 3332\n6: TotalGenMsgSent = 684, rate 684\n7: TotalGenMsgRcvd = 1140, rate 1140\n8: TotalEventMsgSent = 228, rate 228\n9: TotalEventMsgRcvd = 456, rate 456\n10: TotalClientAnnounceReq = 228, rate 228\n11: TotalClientAnnounceReqResend = 0, rate 0\n12: TotalClientAnnounceGrant = 228, rate 228\n13: TotalClientSyncReq = 228, rate 228\n14: TotalClientSyncReqResend = 0, rate 0\n15: TotalClientSyncGrant = 228, rate 228\n16: TotalClientDelayRespReq = 228, rate 228\n17: TotalClientDelayRespReqResend = 0, rate 0\n18: TotalClientDelayRespGrant = 228, rate 228\n19: TotalSyncRcvd = 228, rate 228\n20: TotalPDelayRespRcvd = 0, rate 0\n21: TotalFollowUpRcvd = 228, rate 228\n22: TotalPDelayRespFollowUpRcvd = 0, rate 0\n23: TotalAnnounceRcvd = 228, rate 228\n24: TotalDelayReqSent = 228, rate 228\n25: TotalDelayRespRcvd = 228, rate 228\n26: TotalRetransmitHeapAdd = 0, rate 0\n27: TotalRetransmitHeapAddAlreadyIn = 0, rate 0\n28: TotalRetransmitHeapAddNotIn = 0, rate 0\n29: TotalRetransmitHeapTryRemove = 0, rate 0\n30: TotalRetransmitHeapRemove = 0, rate 0\n31: TotalRetransmitHeapPop = 228, rate 228\nClient states\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\n==Tx Rx Counters=============\nTX worker 0 pkt send: 912\nRX worker 0 pkt recv: 1596\n==Client Request Data============\nAnnounce Grant Requests sent\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\nSync Grant Requests sent\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\nDelay Resp Grant Requests sent\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\nDelay Requests sent\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\nAnnounce Grant Latency\n 228 samples of 228 events\nCumulative: 320.040312ms\nHMean:      1.024843ms\nAvg.:       1.403685ms\np50:        1.16235ms\np75:        1.683062ms\np95:        3.27251ms\np99:        4.523026ms\np999:       5.568122ms\nLong 5%:    4.00771ms\nShort 5%:   471.566\xb5s\nMax:        5.568122ms\nMin:        307.806\xb5s\nRange:      5.260316ms\nStdDev:     888.435\xb5s\nRate/sec.:  712.41\nSync Grant Latency\n 228 samples of 228 events\nCumulative: 300.154828ms\nHMean:      247.17\xb5s\nAvg.:       1.316468ms\np50:        1.18881ms\np75:        1.857054ms\np95:        3.285182ms\np99:        4.467874ms\np999:       5.387614ms\nLong 5%:    3.841688ms\nShort 5%:   42.421\xb5s\nMax:        5.387614ms\nMin:        35.566\xb5s\nRange:      5.352048ms\nStdDev:     1.003697ms\nRate/sec.:  759.61\nDelay Resp Grant Latency\n 228 samples of 228 events\nCumulative: 229.549872ms\nHMean:      145.619\xb5s\nAvg.:       1.006797ms\np50:        1.095026ms\np75:        1.194654ms\np95:        2.357494ms\np99:        3.319574ms\np999:       3.398314ms\nLong 5%:    3.005904ms\nShort 5%:   29.675\xb5s\nMax:        3.398314ms\nMin:        22.114\xb5s\nRange:      3.3762ms\nStdDev:     814.495\xb5s\nRate/sec.:  993.25\nDelay Req Latency\n 228 samples of 228 events\nCumulative: 173.381584ms\nHMean:      127.768\xb5s\nAvg.:       760.445\xb5s\np50:        997.978\xb5s\np75:        1.193058ms\np95:        2.155666ms\np99:        2.336506ms\np999:       3.223606ms\nLong 5%:    2.408821ms\nShort 5%:   29.538\xb5s\nMax:        3.223606ms\nMin:        24.546\xb5s\nRange:      3.19906ms\nStdDev:     672.425\xb5s\nRate/sec.:  1315.02\nTime Between Syncs\n 0 samples of 0 events\nCumulative: 0s\nHMean:      0s\nAvg.:       0s\np50:        0s\np75:        0s\np95:        0s\np99:        0s\np999:       0s\nLong 5%:    0s\nShort 5%:   0s\nMax:        0s\nMin:        0s\nRange:      0s\nStdDev:     0s\nRate/sec.:  0.00\n==Software Performance=============\ntime="2021-08-24T10:46:40-07:00" level=info msg="Profiler RX Worker 0 last busy 0.50%"\ntime="2021-08-24T10:46:40-07:00" level=info msg="Profiler PacketParser 0 last busy 0.50%"\ntime="2021-08-24T10:46:40-07:00" level=info msg="Profiler TX worker 0 last busy 0.50%"\ntime="2021-08-24T10:46:40-07:00" level=info msg="Profiler TX worker 0 TSRead worker 0 last busy 0.50%"\ntime="2021-08-24T10:46:40-07:00" level=info msg="Profiler PacketProcessor 0 last busy 0.50%"\ntime="2021-08-24T10:46:40-07:00" level=info msg="Profiler CounterProcessor last busy 0.00%"\ntime="2021-08-24T10:46:40-07:00" level=info msg="Profiler Client Retransmit Proc 0 last busy 0.00%"\n========================Statistics after 2.005134s============\n==ClientData=============\n0: TotalClients = 228, rate 0\n1: TotalPacketsSent = 1140, rate 228\n2: TotalPacketsRcvd = 1824, rate 228\n3: TotalTXTSPacketsSent = 1140, rate 228\n4: TotalTXTSRead = 1140, rate 228\n5: MaxTXTSBytesOutstanding = 3332, rate 0\n6: TotalGenMsgSent = 684, rate 0\n7: TotalGenMsgRcvd = 1140, rate 0\n8: TotalEventMsgSent = 456, rate 228\n9: TotalEventMsgRcvd = 684, rate 228\n10: TotalClientAnnounceReq = 228, rate 0\n11: TotalClientAnnounceReqResend = 0, rate 0\n12: TotalClientAnnounceGrant = 228, rate 0\n13: TotalClientSyncReq = 228, rate 0\n14: TotalClientSyncReqResend = 0, rate 0\n15: TotalClientSyncGrant = 228, rate 0\n16: TotalClientDelayRespReq = 228, rate 0\n17: TotalClientDelayRespReqResend = 0, rate 0\n18: TotalClientDelayRespGrant = 228, rate 0\n19: TotalSyncRcvd = 228, rate 0\n20: TotalPDelayRespRcvd = 0, rate 0\n21: TotalFollowUpRcvd = 228, rate 0\n22: TotalPDelayRespFollowUpRcvd = 0, rate 0\n23: TotalAnnounceRcvd = 228, rate 0\n24: TotalDelayReqSent = 456, rate 228\n25: TotalDelayRespRcvd = 456, rate 228\n26: TotalRetransmitHeapAdd = 0, rate 0\n27: TotalRetransmitHeapAddAlreadyIn = 0, rate 0\n28: TotalRetransmitHeapAddNotIn = 0, rate 0\n29: TotalRetransmitHeapTryRemove = 0, rate 0\n30: TotalRetransmitHeapRemove = 0, rate 0\n31: TotalRetransmitHeapPop = 456, rate 228\nClient states\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\n==Tx Rx Counters=============\nTX worker 0 pkt send: 1140\nRX worker 0 pkt recv: 1824\n==Client Request Data============\nAnnounce Grant Requests sent\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\nSync Grant Requests sent\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\nDelay Resp Grant Requests sent\nTotal: 228, Max: 1 , Min: 1\nCount 1:228\nDelay Requests sent\nTotal: 228, Max: 2 , Min: 2\nCount 2:228\nAnnounce Grant Latency\n 228 samples of 228 events\nCumulative: 320.040312ms\nHMean:      1.024843ms\nAvg.:       1.403685ms\np50:        1.16235ms\np75:        1.683062ms\np95:        3.27251ms\np99:        4.523026ms\np999:       5.568122ms\nLong 5%:    4.00771ms\nShort 5%:   471.566\xb5s\nMax:        5.568122ms\nMin:        307.806\xb5s\nRange:      5.260316ms\nStdDev:     888.435\xb5s\nRate/sec.:  712.41\nSync Grant Latency\n 228 samples of 228 events\nCumulative: 300.154828ms\nHMean:      247.17\xb5s\nAvg.:       1.316468ms\np50:        1.18881ms\np75:        1.857054ms\np95:        3.285182ms\np99:        4.467874ms\np999:       5.387614ms\nLong 5%:    3.841688ms\nShort 5%:   42.421\xb5s\nMax:        5.387614ms\nMin:        35.566\xb5s\nRange:      5.352048ms\nStdDev:     1.003697ms\nRate/sec.:  759.61\nDelay Resp Grant Latency\n 228 samples of 228 events\nCumulative: 229.549872ms\nHMean:      145.619\xb5s\nAvg.:       1.006797ms\np50:        1.095026ms\np75:        1.194654ms\np95:        2.357494ms\np99:        3.319574ms\np999:       3.398314ms\nLong 5%:    3.005904ms\nShort 5%:   29.675\xb5s\nMax:        3.398314ms\nMin:        22.114\xb5s\nRange:      3.3762ms\nStdDev:     814.495\xb5s\nRate/sec.:  993.25\nDelay Req Latency\n 228 samples of 228 events\nCumulative: 143.902592ms\nHMean:      469.397\xb5s\nAvg.:       631.151\xb5s\np50:        654.054\xb5s\np75:        798.782\xb5s\np95:        1.171114ms\np99:        1.304222ms\np999:       1.486662ms\nLong 5%:    1.282575ms\nShort 5%:   173.455\xb5s\nMax:        1.486662ms\nMin:        130.678\xb5s\nRange:      1.355984ms\nStdDev:     298.877\xb5s\nRate/sec.:  1584.41\nTime Between Syncs\n 0 samples of 0 events\nCumulative: 0s\nHMean:      0s\nAvg.:       0s\np50:        0s\np75:        0s\np95:        0s\np99:        0s\np999:       0s\nLong 5%:    0s\nShort 5%:   0s\nMax:        0s\nMin:        0s\nRange:      0s\nStdDev:     0s\nRate/sec.:  0.00\n==Software Performance=============\ntime="2021-08-24T10:46:41-07:00" level=info msg="Profiler RX Worker 0 last busy 0.00%"\ntime="2021-08-24T10:46:41-07:00" level=info msg="Profiler PacketParser 0 last busy 0.00%"\ntime="2021-08-24T10:46:41-07:00" level=info msg="Profiler TX worker 0 last busy 0.00%"\ntime="2021-08-24T10:46:41-07:00" level=info msg="Profiler TX worker 0 TSRead worker 0 last busy 0.00%"\ntime="2021-08-24T10:46:41-07:00" level=info msg="Profiler PacketProcessor 0 last busy 0.00%"\ntime="2021-08-24T10:46:41-07:00" level=info msg="Profiler CounterProcessor last busy 0.00%"\ntime="2021-08-24T10:46:41-07:00" level=info msg="Profiler Client Retransmit Proc 0 last busy 0.00%"\n')))}d.isMDXComponent=!0}}]);